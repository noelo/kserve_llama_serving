apiVersion: "serving.kserve.io/v1beta1"
kind: "InferenceService"
metadata:
  name: "llama_serving"
spec:
  predictor:
    llama-model:
      protocolVersion: "v1"
      storageUri: "pvc://models/model.onnx"